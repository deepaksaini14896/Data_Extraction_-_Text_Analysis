{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55b80dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "#Read input file into dataframe object.\n",
    "df = pd.read_excel(\"Raw_Data/Input.xlsx\")\n",
    "\n",
    "for i in range(len(df)):\n",
    "    url_id = int(df.iloc[i,0])\n",
    "    \n",
    "    url_link = df.iloc[i,1]\n",
    "    \n",
    "    user_agent = { \"User-Agent\" : \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36\" }\n",
    "    \n",
    "    rqt = requests.get(url_link, headers = user_agent)\n",
    "    \n",
    "    soup = BeautifulSoup(rqt.content, 'html5lib')\n",
    "    \n",
    "    article_title = soup.find('h1', attrs = { \"class\" : \"entry-title\" }).text\n",
    "    \n",
    "    article=soup.find('div', attrs = { \"class\" : \"td-post-content\" })\n",
    "\n",
    "    #Handle exception when pre tag not found in article;\n",
    "    try:\n",
    "    \t#Remove pre tag with the help of extract function\n",
    "        article.find(\"pre\").extract().text\n",
    "    \n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "    article_text = article.text\n",
    "    \n",
    "    #Create a dictionary to collect data.\n",
    "    article_dic = {\n",
    "                    \t\"URL_ID\" : url_id,\n",
    "                    \t\"Article_Title\" : article_title,\n",
    "                    \t\"Article_Text\" : article_text.rstrip().lstrip()\n",
    "                    }\n",
    "    \n",
    "    #Convert dictionary to json format\n",
    "    article_data = json.dumps(article_dic ,indent=4)\n",
    "    \n",
    "    #Create file in Data Extraction folder with the name of url id's.\n",
    "    with open(\"Data_Extraction/\" + str(url_id) + \".txt\", \"w\") as outfile:\n",
    "        outfile.write(article_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eac6e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import copy\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, SyllableTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import xlsxwriter\n",
    "\n",
    "# Create word token of text\n",
    "def create_word_token(article_text):\n",
    "    word = re.sub('[^A-Z]',' ',article_text.upper())\n",
    "    tokenize_word = word_tokenize(word)\n",
    "\n",
    "    return tokenize_word\n",
    "\n",
    "# Remove stop words by using stop word dictionary\n",
    "def remove_stop_word_by_file(tokenize_word, stop_word):\n",
    "    clean_word = list()\n",
    "    \n",
    "    for word in tokenize_word:\n",
    "        if word not in stop_word:\n",
    "            clean_word.append(word)\n",
    "    \n",
    "    return clean_word\n",
    "\n",
    "# Count positive and negative score in master dictionary\n",
    "def positive_negative(clean_word, master_dict):\n",
    "    positive_score = 0\n",
    "    negative_score = 0\n",
    "\n",
    "    positive_word = master_dict.query('Positive != 0')\n",
    "    negative_word = master_dict.query('Negative != 0')\n",
    "    \n",
    "    positive_word = positive_word['Word'].tolist()\n",
    "    negative_word = negative_word['Word'].tolist()\n",
    " \n",
    "    for word in clean_word:\n",
    "        if word in positive_word:\n",
    "            positive_score += 1\n",
    "        elif word in negative_word:\n",
    "            negative_score -= 1\n",
    "    \n",
    "    negative_score *= -1\n",
    "    \n",
    "    return  positive_score, negative_score\n",
    "\n",
    "# Find polarity score using positive and negative score\n",
    "def polarity(positive_score, negative_score):\n",
    "    polarity_score = (positive_score-negative_score)/((positive_score+negative_score)+0.000001)\n",
    "    \n",
    "    return polarity_score\n",
    "\n",
    "# Find subjectivity score using clean word with the help of positive and negative score\n",
    "def subjectivity(positive_score, negative_score, clean_word):\n",
    "    total_clean_word = len(clean_word)\n",
    "    subjectivity_score = (positive_score+negative_score)/((total_clean_word)+0.000001)\n",
    "    \n",
    "    return subjectivity_score\n",
    "    \n",
    "# Count complex word those have more than two syllable using tokenize word\n",
    "def complex_word(tokenize_word):\n",
    "    count = 0\n",
    "    \n",
    "    for word in tokenize_word:\n",
    "        st = SyllableTokenizer()\n",
    "        syllable = st.tokenize(word.lower())\n",
    "        \n",
    "        if len(syllable) > 2:\n",
    "            count += 1\n",
    "    \n",
    "    return count\n",
    "         \n",
    "# Remove stop word using nltk(natural language toolkit)\n",
    "def remove_stop_word_by_nltk(tokenize_word):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    count = 0\n",
    "    \n",
    "    for word in tokenize_word:\n",
    "        if word.lower() not in stop_words:\n",
    "            count += 1\n",
    "    \n",
    "    return count\n",
    "\n",
    "# Count syllable\n",
    "def syllable_count(tokenize_word):\n",
    "    vowels_count = 0\n",
    "    word_count = len(tokenize_word)\n",
    "    vowels = \"AEIOU\"\n",
    "    \n",
    "    for word in tokenize_word:\n",
    "        if len(word) > 1 and word[-2:] in [\"ES\", \"ED\"]:\n",
    "            vowels_count -= 1\n",
    "\n",
    "        for c in word:\n",
    "            if c in vowels:\n",
    "                vowels_count += 1\n",
    "    \n",
    "    syllable_word_per_count = vowels_count/word_count\n",
    "    \n",
    "    return syllable_word_per_count\n",
    "\n",
    "# Count personal pronoun count with the help of re(regular expression)\n",
    "def personal_pronouns_count(article_text):\n",
    "    personal_pronouns_word = re.findall(r\"\\bI\\b|\\bi\\b|\\bWe\\b|\\bwe\\b|\\bMy\\b|\\bmy\\b|\\bOurs\\b|\\bours\\b|\\bus\\b\", article_text)\n",
    "    personal_pronouns = len(personal_pronouns_word)\n",
    "    \n",
    "    return personal_pronouns\n",
    "\n",
    "# Find average word length\n",
    "def average_word(tokenize_word):\n",
    "    total_number_of_word = len(tokenize_word)\n",
    "    total_number_of_character = 0\n",
    "    \n",
    "    for word in tokenize_word:\n",
    "        total_number_of_character += len(word)\n",
    "    \n",
    "    average_word_length = total_number_of_character/total_number_of_word\n",
    "    \n",
    "    return average_word_length \n",
    "\n",
    "# Read input file\n",
    "input_file = pd.read_excel(\"Raw_Data/Input.xlsx\")\n",
    "\n",
    "# Read master dictionary\n",
    "master_dict = pd.read_csv(\"Raw_Data/Loughran-McDonald_MasterDictionary_1993-2021.csv\")\n",
    "\n",
    "# Read stop word dictionary\n",
    "stop_word = pd.read_csv(\"Raw_Data/StopWords_Generic.txt\")\n",
    "\n",
    "# Create output file \n",
    "workbook = xlsxwriter.Workbook('Text_Analysis/Output.xlsx')\n",
    "\n",
    "# Add sheet in workbook\n",
    "worksheet = workbook.add_worksheet()\n",
    "\n",
    "# Create space in column\n",
    "worksheet.set_column(0, 14, 20)\n",
    "\n",
    "# Create column name\n",
    "column_name = [\"URL_ID\", \"URL\", \"POSITIVE SCORE\", \"NEGATIVE SCORE\", \"POLARITY SCORE\", \"SUBJECTIVITY SCORE\",\n",
    "               \"AVG SENTENCE LENGTH\", \"PERCENTAGE OF COMPLEX WORDS\", \"FOG INDEX\", \"AVG NUMBER OF WORDS PER SENTENCE\",\n",
    "              \"COMPLEX WORD COUNT\", \"WORD COUNT\", \"SYLLABLE PER WORD\", \"PERSONAL PRONOUNS\", \"AVG WORD LENGTH\"]\n",
    "\n",
    "# Add bold format in column \n",
    "bold = workbook.add_format({'bold': True})\n",
    "\n",
    "# Write column name\n",
    "worksheet.write_row(0, 0, column_name, bold)\n",
    "\n",
    "\n",
    "for i in range(len(input_file)):\n",
    "    url_id = int(input_file.iloc[i,0])\n",
    "    \n",
    "    url = input_file.iloc[i,1]\n",
    "    \n",
    "    file_object = open(\"Data_Extraction/\" + str(url_id) + \".txt\", \"r\")\n",
    "    \n",
    "    file_data = file_object.read()\n",
    "    \n",
    "    file_dic = json.loads(file_data)\n",
    "    \n",
    "    article_text = file_dic[\"Article_Text\"]\n",
    "    \n",
    "    tokenize_word = create_word_token(article_text)\n",
    "    \n",
    "    number_word = len(tokenize_word)\n",
    "    \n",
    "    number_sentence = len(sent_tokenize(article_text))\n",
    "    \n",
    "    clean_word = remove_stop_word_by_file(tokenize_word, stop_word)\n",
    "  \n",
    "    positive_score, negative_score = positive_negative(clean_word, master_dict)\n",
    "    \n",
    "    polarity_score = polarity(positive_score, negative_score)\n",
    "    \n",
    "    subjectivity_score = subjectivity(positive_score, negative_score, clean_word)    \n",
    "    \n",
    "    avg_sentence_length = number_word/number_sentence\n",
    "    \n",
    "    complex_word_count = complex_word(tokenize_word)\n",
    "    \n",
    "    percentage_of_complex_words = 100*(complex_word_count/number_word)\n",
    "\n",
    "    fog_index = 0.4*(avg_sentence_length+percentage_of_complex_words)\n",
    "    \n",
    "    avg_number_of_words_per_sentence = number_word/number_sentence\n",
    "\n",
    "    word_count = remove_stop_word_by_nltk(tokenize_word)\n",
    "    \n",
    "    syllable_per_word = syllable_count(tokenize_word)\n",
    "    \n",
    "    personal_pronouns = personal_pronouns_count(article_text)\n",
    "    \n",
    "    avg_word_length = average_word(tokenize_word)\n",
    "    \n",
    "    worksheet.write_row(i+1, 0, [url_id, url, positive_score, negative_score, polarity_score, subjectivity_score,\n",
    "                               avg_sentence_length, percentage_of_complex_words, fog_index, avg_number_of_words_per_sentence,\n",
    "                               complex_word_count, word_count, syllable_per_word, personal_pronouns, avg_word_length])\n",
    "    \n",
    "# Close workbook\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cefac509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df =  pd.read_excel(\"Raw_Data/Input.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5be749c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://insights.blackcoffer.com/how-is-login-logout-time-tracking-for-employees-in-office-done-by-ai/\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8ae47b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['a','b','c','d','e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07db3da4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can only merge Series or DataFrame objects, a <class 'list'> was passed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12249/4109588149.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/Blackcoffer/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m   9188\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9190\u001b[0;31m         return merge(\n\u001b[0m\u001b[1;32m   9191\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9192\u001b[0m             \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Blackcoffer/lib/python3.9/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m ) -> DataFrame:\n\u001b[0;32m--> 106\u001b[0;31m     op = _MergeOperation(\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Blackcoffer/lib/python3.9/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    626\u001b[0m     ):\n\u001b[1;32m    627\u001b[0m         \u001b[0m_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_operand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m         \u001b[0m_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_operand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_left\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_right\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/Blackcoffer/lib/python3.9/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_validate_operand\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2274\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2276\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m   2277\u001b[0m             \u001b[0;34mf\"Can only merge Series or DataFrame objects, a {type(obj)} was passed\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2278\u001b[0m         )\n",
      "\u001b[0;31mTypeError\u001b[0m: Can only merge Series or DataFrame objects, a <class 'list'> was passed"
     ]
    }
   ],
   "source": [
    "df.merge(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2424d152",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
